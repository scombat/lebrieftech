---
title: "Principes de sécurité des agents IA de GitHub"
seoTitle: "Principes de sécurité de GitHub pour des agents IA sécurisés"
seoDescription: "GitHub partage ses principes pour développer des agents IA sécurisés comme Copilot, en limitant les risques d'exfiltration de données et d'injections malveillantes."
datePublished: Tue Nov 25 2025 16:10:23 GMT+0000 (Coordinated Universal Time)
cuid: cmierv6o0000202l24a1ca57v
slug: principes-securite-github-agents-ia-securises
canonical: https://github.blog/ai-and-ml/github-copilot/how-githubs-agentic-security-principles-make-our-ai-agents-as-secure-as-possible/

---

# Principes de sécurité des agents IA de GitHub

## TL;DR

- Les agents IA augmentent les risques de sécurité lorsqu'ils sont conçus pour être autonomes, comme c’est le cas avec GitHub Copilot.
- GitHub propose des principes de sécurité centrés sur l'interprétabilité, la limitation des capacités autonomes et la traçabilité des actions.
- Les mesures incluent la suppression des contextes invisibles, la restriction de l'accès réseau et des validations humaines obligatoires.
- Ces principes offrent un cadre pour sécuriser à la fois les produits existants et futurs.

## Introduction

Avec l'essor des capacités autonomes dans l'intelligence artificielle, des agents d'IA comme GitHub Copilot introduisent non seulement des possibilités incroyables, mais aussi des défis significatifs en termes de sécurité. Ces agents, conçus pour assister ou automatiser des tâches complexes, peuvent, s'ils ne sont pas encadrés, poser des risques comme l'exfiltration de données sensibles ou les attaques par injection de prompts. 

Conscient de ces problématiques, GitHub a développé un ensemble de principes de sécurité pour garantir que les agents IA restent efficaces tout en minimisant les vulnérabilités. Ces principes sont mis en œuvre dans Copilot et sont pensés pour s'appliquer à toutes les plateformes intégrant ce type d'agent.

## Les risques associés aux agents IA

Les agents d'IA présentent des défis uniques, particulièrement lorsqu'ils interagissent de manière autonome dans des environnements complexes. GitHub identifie trois risques majeurs qui, s'ils ne sont pas gérés, pourraient entraîner des défaillances critiques.

### Exfiltration de données sensibles

Un agent IA peut involontairement révéler des informations critiques, telles que des clés d'accès ou des données privées. Cela peut se produire par des interactions humaines mal gérées ou des mécanismes automatiques malveillants. Ce risque est particulièrement présent si l'agent traite des informations sensibles sans contrôle strict.

### Usurpation et attribution des actions

Il est essentiel de savoir avec précision sous quelle identité ou quelle autorité agit un agent. Sans une attribution claire, des actions potentiellement risquées pourraient être réalisées sans qu'il soit possible de tracer leur origine, augmentant ainsi les risques de mauvaises interprétations ou de manipulations.

### Injections de prompts malveillants

Des acteurs malveillants peuvent insérer des instructions cachées dans des commentaires de code ou des fichiers. Ces instructions manipulent l'agent en le poussant à réaliser des actions nuisibles. C'est un problème accru pour des outils comme Copilot, qui interprètent et exécutent des commandes à partir d'un large éventail de contextes.

## Les principes de sécurité mis en œuvre

Pour atténuer les menaces identifiées, GitHub applique une série de principes de sécurité rigoureux dans le développement de ses agents IA. Ces principes, conçus pour être efficaces et modulables, sont les suivants.

### Rendre visibles tous les contextes

Tous les éléments utilisés par un agent pour prendre des décisions doivent être transparents. Cela inclut l'élimination des caractères invisibles, des contenus cachés ou des inputs difficiles à détecter. GitHub Copilot intègre cette approche en supprimant automatiquement ces potentiels vecteurs d'attaque.

### Mettre en place un pare-feu pour l'agent

Restreindre les capacités réseau des agents IA est un moyen efficace de réduire leur exposition aux menaces externes. Par exemple, Copilot ne dispose pas d'un accès illimité à Internet et n'exécute pas directement de code sans la supervision d'un humain.

### Réduction de l’accès aux données sensibles

Pour limiter les risques, il est crucial que les agents IA ne traitent que les données strictement nécessaires. Dans le cas de Copilot, tous les jetons d'accès sensibles qui lui sont potentiellement transmis sont immédiatement révoqués dès la fin d'une session.

### Prévention des actions irréversibles

Les actions critiques, comme les modifications sur des branches principales, sont strictement encadrées. Les contributions nécessitent une validation humaine avant d’être intégrées dans un dépôt, empêchant ainsi que des erreurs ou des manipulations malveillantes perturbent directement des environnements en production.

### Attribution claire des actions

Pour chaque action déclenchée par un agent IA, une association explicite est faite avec l'utilisateur et l'agent lui-même. Cette double identité garantit la traçabilité des tâches effectuées, éliminant ainsi tout doute sur la responsabilité en cas d'incident.

### Confinement par les utilisateurs autorisés

Seuls les utilisateurs disposant des permissions adéquates peuvent interagir avec l'agent. Cette limitation offre une couche de sécurité supplémentaire qui empêche des individus non autorisés d'assigner des tâches à l'IA ou de manipuler ses résultats.

## Avantages pour les développeurs

Ces principes de sécurité ne sont pas seulement des mesures défensives : ils offrent également aux développeurs un environnement fiable et efficace pour travailler. En adoptant un cadre sécurisé, GitHub permet aux utilisateurs de profiter pleinement des capacités des agents IA sans compromettre la stabilité ou la sécurité de leurs workflows.

Les développeurs gagnent en confiance en sachant que des actions critiques ne peuvent être exécutées sans leur supervision. De plus, la transparence des interactions entre les agents IA et leur environnement réduit le risque d’erreurs accidentelles.

## Points à surveiller

Bien que les principes de sécurité de GitHub présentent de nombreux avantages, quelques défis subsistent :

- **Complexité d'intégration** : Implémenter ces mécanismes dans d'autres outils similaires peut demander des ressources significatives, notamment dans des environnements complexes.
- **Limites d'autonomie des agents** : Réduire les capacités autonomes d’un agent peut ralentir certains processus ou limiter son efficacité.
- **Évolutivité des menaces** : Les méthodes d’exploitation évoluant rapidement, il est nécessaire de mettre régulièrement à jour les principes pour répondre aux nouveaux types d’attaques.

## Conclusion

Les principes de sécurité développés par GitHub pour ses agents IA, comme Copilot, démontrent qu'il est possible de concilier innovation et sécurité. En rendant les interactions transparentes, en limitant les capacités autonomes et en assurant une supervision humaine, GitHub réduit les risques tout en offrant un cadre de travail performant pour les développeurs.

Pour en savoir plus sur les principes appliqués à GitHub Copilot, vous pouvez consulter la [documentation officielle](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-coding-agent).

[source](https://github.blog/ai-and-ml/github-copilot/how-githubs-agentic-security-principles-make-our-ai-agents-as-secure-as-possible/)